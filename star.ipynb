{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STaR Pipeline class object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lmdeploy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getenv \n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlmdeploy\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Model & LLM \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# LMDeploy + OpenAI (Student & Teacher Pair)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# model_id = \"Ksgk-fy/ecoach_phil_v11_3\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# pipe = lmdeploy.pipeline(model_id)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m API\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lmdeploy'"
     ]
    }
   ],
   "source": [
    "# Get deployed llm to generate responses, then use OAI to correct and supervise the rationale\n",
    "# --- we focus on the wrong ones \n",
    "# Self-Taught Reasoner for association enhancement \n",
    "import random\n",
    "from openai import OpenAI \n",
    "from os import getenv \n",
    "import json\n",
    "import lmdeploy\n",
    "\n",
    "# Model & LLM \n",
    "# LMDeploy + OpenAI (Student & Teacher Pair)\n",
    "# model_id = \"Ksgk-fy/ecoach_phil_v11_3\"\n",
    "# pipe = lmdeploy.pipeline(model_id)\n",
    "API_KEY = getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def get_oai_response(prompt, system_prompt, oai_client):\n",
    "    response = oai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # Use an appropriate OpenAI model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def get_lmdeploy_response(prompts, pipe):\n",
    "    \"\"\" \n",
    "    Batch inference with LMDeploy package, local LLM\n",
    "    \"\"\"\n",
    "    import asyncio\n",
    "    import nest_asyncio\n",
    "    \n",
    "    async def run_inference():\n",
    "        responses = pipe(prompts)\n",
    "        return [response.text for response in responses]\n",
    "    \n",
    "    # Apply nest_asyncio to allow running event loop in Jupyter\n",
    "    nest_asyncio.apply()\n",
    "    \n",
    "    return asyncio.run(run_inference())\n",
    "\n",
    "\n",
    "def get_vllm_response(prompts, llm):\n",
    "    \"\"\" \n",
    "    Batch Inference with vLLM, local LLM\n",
    "    \"\"\"\n",
    "    import asyncio\n",
    "    import nest_asyncio\n",
    "    \n",
    "    async def run_inference():\n",
    "        responses = llm.generate(prompts)\n",
    "        return [response.text for response in responses]\n",
    "    \n",
    "    # Apply nest_asyncio to allow running event loop in Jupyter\n",
    "    nest_asyncio.apply()\n",
    "    \n",
    "    return asyncio.run(run_inference())\n",
    "    \n",
    "\n",
    "# Util function\n",
    "def parse_rationale_answer(response):\n",
    "    \"\"\"\n",
    "    Parse out Rationale and Answer from the response text.\n",
    "    Returns False, False if parsing fails.\n",
    "    \"\"\"\n",
    "    rationale, answer = False, False\n",
    "    try:\n",
    "        ration_suffix = response.split(\"Rationale: \")[1]\n",
    "        rationale = ration_suffix.split(\"Answer: \")[0].strip()\n",
    "        answer_txt = ration_suffix.split(\"Answer: \")[1].split(\"\\n\")[0].strip()\n",
    "        if \"a.\" in answer_txt.lower() or \"a\" == answer_txt:\n",
    "            answer = \"a\"\n",
    "        elif \"b.\" in answer_txt.lower() or \"b\" == answer_txt:\n",
    "            answer = \"b\"\n",
    "        elif \"c.\" in answer_txt.lower() or \"c\" == answer_txt:\n",
    "            answer = \"c\"\n",
    "        elif \"d.\" in answer_txt.lower() or \"d\" == answer_txt:\n",
    "            answer = \"d\"        \n",
    "    except:\n",
    "        pass\n",
    "    return rationale, answer\n",
    "\n",
    "\n",
    "roleplay_prompt = \"Roleplay as Maria, a Filipina woman having a conversation with an FWD insurance agent.\"\n",
    "\n",
    "\n",
    "class STaRDatapoint:\n",
    "    \n",
    "    def __init__(self, question, answer_choices, correct_answer, pipe, oai_client):\n",
    "        self.question = question\n",
    "        self.answer_choices = answer_choices\n",
    "        self.correct_answer = correct_answer\n",
    "        self.generated_rationale = \"\"\n",
    "        self.generated_answer = \"\"\n",
    "        self.oai_client = oai_client\n",
    "        self.pipe = pipe\n",
    "        self.correct_rationales = [] # Record of all the correct rationales\n",
    "\n",
    "    def generate_rationale_and_answer(self, use_lm=False):\n",
    "        system_prompt = f\"{roleplay_prompt} You are Maria, responding to queries from an FWD insurance agent.\"\n",
    "        user_prompt = f\"FWD agent asks: {self.question}\\nPossible responses: {self.answer_choices}\\nAs Maria, provide your rationale and choose an answer. Your response should be in the format:\\nRationale: [Your rationale here]\\nAnswer: [Single letter a/b/c/d corresponding to your chosen response]\"\n",
    "        \n",
    "        if use_lm:\n",
    "            full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "            response = get_lmdeploy_response([full_prompt], self.pipe)[0]\n",
    "        else:\n",
    "            response = get_oai_response(user_prompt, system_prompt, self.oai_client)\n",
    "\n",
    "        rationale, answer = parse_rationale_answer(response)\n",
    "        self.generated_rationale = rationale\n",
    "        self.generated_answer = answer \n",
    "        print(\"Generated Answer: \", answer)\n",
    "        return response\n",
    "\n",
    "    def check_answer(self):\n",
    "        is_valid = self.generated_answer and self.generated_rationale\n",
    "        is_correct = self.generated_answer.lower() == self.correct_answer.lower()\n",
    "        return is_valid and is_correct \n",
    "    \n",
    "    def generate(self, n, use_lm=False):\n",
    "        \"\"\" \n",
    "        Generate n (rationale, answer) tuples\n",
    "        this will be used to supervise on a LLM to be able to learn the rationale and picking the correct answer\n",
    "        \"\"\"\n",
    "        generated_data = []\n",
    "        for _ in range(n):\n",
    "            # Generate a random question and answer choices\n",
    "            question = f\"Random question {_+1}\"\n",
    "            answer_choices = [f\"Option {chr(97+i)}\" for i in range(4)]  # a, b, c, d\n",
    "            correct_answer = random.choice(['a', 'b', 'c', 'd'])\n",
    "            \n",
    "            # Generate rationale and answer\n",
    "            self.question = question\n",
    "            self.answer_choices = answer_choices\n",
    "            self.correct_answer = correct_answer\n",
    "            self.generate_rationale_and_answer(use_lm)\n",
    "            \n",
    "            # Append to the list\n",
    "            generated_data.append((self.generated_rationale, self.generated_answer))\n",
    "        \n",
    "        return generated_data\n",
    "    \n",
    "    \n",
    "class STaRPipeline:\n",
    "    \n",
    "    def __init__(self, model_id, roleplay_prompt, data, num_rationales=5):\n",
    "        self.model_id = model_id\n",
    "        self.pipe = lmdeploy.pipeline(model_id)\n",
    "        self.oai_client = OpenAI(api_key=API_KEY)\n",
    "        self.roleplay_prompt = roleplay_prompt\n",
    "        self.num_rationales = num_rationales\n",
    "\n",
    "        # Initialize STaR datapoints here\n",
    "        self.datapoints = [STaRDatapoint(q, ac, a, self.pipe, self.oai_client) \n",
    "                           for q, ac, a in zip(data[\"question\"], data[\"answer_choices\"], data[\"answer\"])]\n",
    "\n",
    "    def process_datapoints(self):\n",
    "        for datapoint in self.datapoints:\n",
    "            # 1. Use the to-be-trained model to generate rationale and answer\n",
    "            response = datapoint.generate_rationale_and_answer(use_lm=True)\n",
    "            \n",
    "            # 2. Check if the answer is correct\n",
    "            if not datapoint.check_answer():\n",
    "                print(\"Wrong Answer\")\n",
    "                # If wrong, use strong LLM (OpenAI) to generate multiple rationales\n",
    "                for _ in range(self.num_rationales):\n",
    "                    # Prompt the strong LLM (OpenAI) to provide rationale for the correct answer\n",
    "                    prompt = f\"\"\"\n",
    "                    Question: {datapoint.question}\n",
    "                    \n",
    "                    The correct answer is: {datapoint.answer_choices[ord(datapoint.correct_answer) - ord('a')][3:]}\n",
    "                    \n",
    "                    Please provide a concise rationale explaining why this is the correct answer.\n",
    "                    \"\"\"\n",
    "                    response = get_oai_response(prompt, system_prompt=\"You are an expert in reasoning.\", oai_client=self.oai_client)\n",
    "                    \n",
    "                    # Extract the rationale from the response\n",
    "                    strong_rationale = response\n",
    "                    \n",
    "                    # Update the datapoint with the strong rationale and correct answer\n",
    "                    datapoint.generated_rationale = strong_rationale\n",
    "                    datapoint.generated_answer = datapoint.correct_answer\n",
    "                    \n",
    "                    # Add the rationale to correct_rationales\n",
    "                    datapoint.correct_rationales.append(strong_rationale)\n",
    "                    print(\"Strong Rationale: \", strong_rationale)\n",
    "                    \n",
    "            else:\n",
    "                # 3. If correct, record the rationale\n",
    "                datapoint.correct_rationales.append(datapoint.generated_rationale)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "question = \"Tell me about the products you have at FWD\"\n",
    "answer_choices = [\n",
    "    \"a. Sure! FWD offers a range of insurance products, including life insurance, health insurance, and investment-linked insurance plans.\",\n",
    "    \"b. FWD has a variety of insurance products that could help you in different life stages\",\n",
    "    \"c. Absolutely, let me introduce FWD products to you!\",\n",
    "    \"d. I don't know what product FWD offer, I am expecting you to tell me about them, you are the sales here.\"\n",
    "]\n",
    "correct_answer = \"d\"\n",
    "\n",
    "question_list = [question]\n",
    "answer_choices_list = [answer_choices]\n",
    "answer_list = [correct_answer]\n",
    "\n",
    "data = {\n",
    "    \"question\": question_list,\n",
    "    \"answer_choices\": answer_choices_list,\n",
    "    \"answer\": answer_list\n",
    "}\n",
    "\n",
    "pipe = STaRPipeline(\n",
    "    model_id=\"Ksgk-fy/ecoach_phil_v11_3\", \n",
    "    roleplay_prompt=\"Roleplay as Maria, a Filipina woman having a conversation with an FWD insurance agent.\",\n",
    "    data=data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
